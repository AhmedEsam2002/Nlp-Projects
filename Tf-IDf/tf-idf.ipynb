{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "cell_execution_strategy": "setup"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ub2WYF3BKaMf"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from transformers import pipeline\n",
        "import numpy as np\n",
        "import numpy as np\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "import re\n",
        "import nltk\n",
        "\n",
        "# Download necessary NLTK datasets\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Preprocessing function\n",
        "def preprocess_document(document):\n",
        "    document = document.lower()\n",
        "    document = re.sub(r'[^a-zA-Z\\s]', ' ', document)\n",
        "    tokens = word_tokenize(document)\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    tokens = [lemmatizer.lemmatize(word) for word in tokens if word not in stopwords.words('english')]\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    filtered_tokens = [word for word in tokens if word not in stop_words]\n",
        "    return ' '.join(filtered_tokens)\n"
      ],
      "metadata": {
        "id": "7lw0JggVp4KP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CustomTFIDF:\n",
        "    def __init__(self):\n",
        "        self.idf_ = {}\n",
        "        self.vocab_ = {}\n",
        "\n",
        "    def fit_transform(self, documents):\n",
        "        tf = []\n",
        "        doc_count = len(documents)\n",
        "\n",
        "        # Compute term frequencies and document frequencies for IDF\n",
        "        for document in documents:\n",
        "            doc_tf = {}\n",
        "            words = document.split()\n",
        "            for word in words:\n",
        "                doc_tf[word] = doc_tf.get(word, 0) + 1\n",
        "            for word in doc_tf:\n",
        "                doc_tf[word] = doc_tf[word] / len(words)\n",
        "                self.idf_[word] = self.idf_.get(word, 0) + 1\n",
        "            tf.append(doc_tf)\n",
        "\n",
        "        # Sort the vocabulary alphabetically and assign indices\n",
        "        sorted_vocab = sorted(self.idf_.keys())\n",
        "        self.vocab_ = {word: idx for idx, word in enumerate(sorted_vocab)}\n",
        "\n",
        "        # Compute IDF using the sorted vocabulary\n",
        "        for word in self.idf_:\n",
        "            self.idf_[word] = np.log((1 + doc_count) / (1 + self.idf_[word])) + 1\n",
        "\n",
        "        # Compute TF-IDF scores using the sorted vocabulary\n",
        "        tfidf = []\n",
        "        for doc in tf:\n",
        "            doc_tfidf = np.zeros(len(self.vocab_))\n",
        "            for word, value in doc.items():\n",
        "                if word in self.vocab_:\n",
        "                    index = self.vocab_[word]\n",
        "                    doc_tfidf[index] = value * self.idf_[word]\n",
        "            # L2 Normalization\n",
        "            norm = np.linalg.norm(doc_tfidf)\n",
        "            if norm > 0:\n",
        "                doc_tfidf = doc_tfidf / norm\n",
        "            tfidf.append(doc_tfidf)\n",
        "\n",
        "        return np.array(tfidf)"
      ],
      "metadata": {
        "id": "XjFrLG5sKbcp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Initialize pipeline for text generation\n",
        "generator = pipeline('text-generation', model ='EleutherAI/gpt-neo-1.3B')\n",
        "\n",
        "# Define prompts for text generation\n",
        "prompts = [\n",
        "    \"The industrial revolution and its consequences have been a disaster for the human race.\",\n",
        "    \"Social media is a great inhouse threat for families that could result in long term harm.\",\n",
        "    \"The fall of the great umayyad caliphate had negative effects on its territories.\"\n",
        "]\n"
      ],
      "metadata": {
        "id": "yMjj0CRFKoPU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate texts based on prompts and save to files\n",
        "for i, prompt in enumerate(prompts, 1):\n",
        "    output = generator(prompt, max_length=100, do_sample=True, temperature=0.9)\n",
        "    with open(f'output_{i}.txt', 'w') as f:\n",
        "        f.write(str(output))\n"
      ],
      "metadata": {
        "id": "tfVBS3pDKueG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Read generated texts from files\n",
        "docs = []\n",
        "for filename in os.listdir('.'):\n",
        "    if filename.startswith(\"output_\") and filename.endswith(\".txt\"):\n",
        "        with open(filename, \"r\")  as file:\n",
        "            document = file.read()\n",
        "            document=preprocess_document(document)\n",
        "            docs.append(document)\n"
      ],
      "metadata": {
        "id": "hMAIoqRdLI2O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "# Using sklearn for comparison\n",
        "tfidf_vectorizer = TfidfVectorizer()\n",
        "sklearn_tfidf_matrix = tfidf_vectorizer.fit_transform(docs).toarray()"
      ],
      "metadata": {
        "id": "XckeWfCwOFZ1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Using CustomTFIDF\n",
        "custom_tfidf = CustomTFIDF()\n",
        "custom_tfidf_matrix = custom_tfidf.fit_transform(docs)"
      ],
      "metadata": {
        "id": "iBsnJCURSTd7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Comparing results (simplified, for detailed comparison, iterate over matrices)\n",
        "print(\"Custom TF-IDF vs. sklearn TF-IDF (first document vector):\")\n",
        "print(\"Custom:\", custom_tfidf_matrix[1].reshape(-1,1))"
      ],
      "metadata": {
        "id": "JZ-iHkbDToLM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"sklearn:\", sklearn_tfidf_matrix[1].reshape(-1,1))"
      ],
      "metadata": {
        "id": "eYfhJ0UHTs-s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "VxIlomaGTw6W"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}